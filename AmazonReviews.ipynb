{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # needed for Python 2\n",
    "from __future__ import division        # needed for Python 2\n",
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always') # always show warnings, useful for SK Learning warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/text/label tuple\n",
    "# returns a triple of an integer (ID), a string containing the review, and a string indicating the label\n",
    "# DOC_ID\tLABEL\tRATING\tVERIFIED_PURCHASE\tPRODUCT_CATEGORY\tPRODUCT_ID\tPRODUCT_TITLE\tREVIEW_TITLE\tREVIEW_TEXT\n",
    "def parseReview(reviewLine):\n",
    "    # reviewLine[0] = DOC_ID\n",
    "    # reviewLine[8] = REVIEW_TEXT\n",
    "    # reviewLine[1] = LABEL\n",
    "    return (reviewLine[0], reviewLine[8], reviewLine[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# preProcess - process one sentence and return a list of processed tokens\n",
    "# includes various normalisation options\n",
    "#\n",
    "# parameters:\n",
    "# @lower - convert all text to lowercase\n",
    "# @punc - remove punctuation\n",
    "# @stem - simplify into stems using NLTK\n",
    "# @stop - remove stop words using either SKLearn = 1 or NLTK = 2\n",
    "# @lemma - lemmatize with NLTK\n",
    "\n",
    "def preProcess(text, lower=1, punc=1, stem=1, stop=2, lemma=0):\n",
    "    \n",
    "    # remove punctuation from our list of tokens\n",
    "    if (punc == 1):\n",
    "        text = text.replace('<br />', '') # remove HTML newline tags\n",
    "        text = text.replace('&#34;', ' ') # remove quotation mark &#34; encoding\n",
    "        text = text.replace(')', ' ') # replace brackets with empty space\n",
    "        text = text.replace('(', ' ')\n",
    "        text = text.replace('.', ' ')\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    \n",
    "    # ensure our tokens are all lowercase\n",
    "    if (lower == 1):    \n",
    "        text = text.lower()  \n",
    "        \n",
    "    tokens = text.split() # split by whitespace\n",
    "    \n",
    "    # remove stem words using NLTK\n",
    "    if (stem == 1):\n",
    "        tokens = stemTokens(tokens)\n",
    "    \n",
    "    # remove stop words using sklearn\n",
    "    if (stop == 1):\n",
    "        tokens = stopTokens(tokens, 1)\n",
    "        \n",
    "    # remove stop words using NLTK\n",
    "    if (stop == 2):\n",
    "        tokens = stopTokens(tokens, 2)\n",
    "        \n",
    "    if (lemma == 1):\n",
    "        tokens = lemmaTokens(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# simplify words into their stems using the SnowballStemmer from NLTK\n",
    "def stemTokens(tokens):\n",
    "    new_tokens = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for token in tokens:\n",
    "        new_tokens.append(stemmer.stem(token))\n",
    "    return new_tokens\n",
    "\n",
    "# remove stop words\n",
    "# lib = 1, use SKLearn\n",
    "# lib = 2, use NLTK\n",
    "def stopTokens(tokens, lib):\n",
    "    \n",
    "    # remove using SKLearn\n",
    "    # list of removed words: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py\n",
    "    if (lib == 1):\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "        x = tfidf_vectorizer.fit_transform(tokens)\n",
    "        new_tokens = tfidf_vectorizer.get_feature_names()\n",
    "        \n",
    "    # remove stop words using NLTK\n",
    "    # type set(stopwords.words('english')) in interpreter to see list of words\n",
    "    if (lib == 2):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        new_tokens = [w for w in tokens if not w in stop_words] \n",
    "        \n",
    "    return new_tokens\n",
    "\n",
    "# Lemmatization of tokens with NLTK\n",
    "def lemmaTokens(tokens):\n",
    "    new_tokens = []\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    for token in tokens:\n",
    "        new_tokens.append(lemmatizer.lemmatize(token, pos=\"v\")) # lemmatize verbs, e.g. running to run\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    featureVector = {}\n",
    "    for token in tokens:\n",
    "        \n",
    "        # feature dictionary\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1\n",
    "        else:\n",
    "            featureVector[token] += 1\n",
    "        \n",
    "        ''' Binary feature weighting\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1\n",
    "        '''\n",
    "        \n",
    "        # global dictionary\n",
    "        if token not in featureDict:\n",
    "            featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] += 1\n",
    "            \n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path, newline='', encoding=\"latin-1\") as f: \n",
    "        # use encoding=\"latin-1\" encoding to fix Unicode decode errors?\n",
    "        next(f) # skip the first line (headers)\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            #print(line)\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "            preprocessedData.append((Id, preProcess(Text), Label))\n",
    "        print(\"All data loaded from \" + path)\n",
    "        \n",
    "def processVector():\n",
    "    for (_, Text, Label) in rawData:\n",
    "        vectoredProcessed.append((toFeatureVector(preProcess(Text)),Label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData\n",
      "Preparing the dataset...\n",
      "All data loaded from amazon_reviews.txt\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "# reset the featuredict\n",
    "featureDict = {}\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "vectoredProcessed = []  # our completed data, with text to vectors and preprocessed\n",
    "\n",
    "# references to the data files\n",
    "#reviewPath = '100_reviews.txt' # a smaller set of the data for fast iteration\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData\" % (len(rawData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(max_iter=10000))],verbose=False)\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-FOLD CROSS VALIDATION\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset) # shuffle only once before implementing k-folds to ensure distribution of fake/real in dataset\n",
    "    cv_results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        crossTest = dataset[i:i+foldSize] # from start of our heldout up to the fold size\n",
    "        crossTrain = dataset[:i] + dataset[i+foldSize:] # from start to start of heldout, and also from the end of heldout up to end of set\n",
    "        \n",
    "        # train on the trainData\n",
    "        classifier = trainClassifier(crossTrain)\n",
    "        \n",
    "        # predict on the testData with the classifier\n",
    "        crossPredictions = predictLabels(crossTest, classifier)\n",
    "        \n",
    "        # return the actual labels for the testData\n",
    "        crossActual = [x[1] for x in crossTest] # list comprehension to take only real/fake label from testData\n",
    "        \n",
    "        # use sklearn.metrics to see how our predictions did\n",
    "        # provide a weighted score to account for imbalance between labels\n",
    "        result = precision_recall_fscore_support(crossActual, crossPredictions, average='weighted')\n",
    "        \n",
    "        cv_results.append(result)\n",
    "        continue\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to reset our processed data and featureDict after changing settings of toFeatureVector/preProcess()\n",
    "featureDict = {}\n",
    "vectoredProcessed = []\n",
    "\n",
    "processVector() # apply toFeatureVector() and preProcess() on the rawData, populate vectoredProcessed to cross validate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Average:  [0.610302720332182, 0.6099047619047618, 0.6096648959279517]\n"
     ]
    }
   ],
   "source": [
    "cv_results = crossValidate(vectoredProcessed, 10)\n",
    "# print(cv_results)\n",
    "\n",
    "total = [0,0,0]\n",
    "for x in cv_results:\n",
    "    total[0] += x[0] # precision\n",
    "    total[1] += x[1] # recall\n",
    "    total[2] += x[2] # f score\n",
    "\n",
    "# averages\n",
    "total[0] = total[0]/len(cv_results)\n",
    "total[1] = total[1]/len(cv_results)\n",
    "total[2] = total[2]/len(cv_results)\n",
    "\n",
    "print(\"Average: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average results over 10-fold cross validation\n",
    "` Average: [precision, recall, fscore]`\n",
    "\n",
    " 1. with basic split() based on whitespace (features: 89115)\n",
    "\n",
    "> Average:  [0.6055228430471471, 0.6049404761904762, 0.6048509385677797]\n",
    "\n",
    " 2. converted to lowercase, punctuation removed (features: 41397)\n",
    " \n",
    "> Average:  [0.6063646761606878, 0.6058333333333333, 0.6056341871532889]\n",
    "\n",
    "\n",
    " 3. converted to lowercase, punctuation removed, removing stop words with NLTK, stemming words with NLTK (features: 30015)\n",
    " \n",
    "> Average:  [0.6112074377868859, 0.6105357142857143, 0.6102143961844734]\n",
    "\n",
    "\n",
    " 4. converted to lowercase, punctuation removed, removing stop words with NLTK, stemming words with SKLearn (features: 29760)\n",
    "\n",
    "> Average:  [0.6040157712422838, 0.6033928571428572, 0.6030488493264483]\n",
    " \n",
    " \n",
    " 5. converted to lowercase, punctuation removed, removing stop words with NLTK, lemmatizing with NLTK (verb)\n",
    " \n",
    "> Average:  [0.6041187396998227, 0.60375, 0.6034261476908608]\n",
    "\n",
    "We tidied up the words removing punctuation, various tokens in the text such as `<br />` and `&#34;` This reduced the feature count and had a very minor improvement.\n",
    "\n",
    "We had improved results using stemming with NLTK over lemmatization. We were unable to properly fully lemmatize since we did not have morphological information for each word (e.g. noun, verb). As such we lemmatized each word assuming it was a verb so \"running\" would produce \"run\" for example. The NLTK stemmer performed much better than the SKLearn stemmer.\n",
    "\n",
    "Removing stop words and stemming with NLTK had the best effect overall, although still a relatively minor improvement from the baseline whitespace.\n",
    "\n",
    "# Feature weighting\n",
    "\n",
    "Feature weighting was done based on the count of tokens in a given sentence. We compared our best result above with the same preProcess settings, however with feature weighting as a binary option.\n",
    "\n",
    "Binary feature weighting: \n",
    "> Average: [0.6011458453150506, 0.6008928571428572, 0.6006165656925394]\n",
    "\n",
    "vs feature count weighting: \n",
    "> Average: [0.6112074377868859, 0.6105357142857143, 0.6102143961844734]\n",
    "\n",
    "Feature count weighting does improve the performance although we are still only talking about an improvement of 1/100th so relatively minor again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including other features\n",
    "\n",
    "The next stage is to include other features such as rating and verified purchase from the data to see if this improves prediction.\n",
    "\n",
    "We will add RATING, VERIFIED_PURCHASE and PRODUCT_TITLE. I posit verified purchases may be more likely to be true since there is a cost involved. I would expect a large amount of fake reviews to be non-verified. I have also chosen rating as I also suspect most fake reviews will be 5 star so this should have good predictive power. Intuitively I would suspect certain categories of product are more likely to have fake reviews posted for them and so we could use that to predict more accuractely. However Amazon has provided us with equal distributions of fake/real (350 each) in the same categories, so while this may have some preditive power combined with other features, alone it is unlikely to predict much. As such I have chosen the product title as the final feature as this can have a similar predictive power. I expect certain types of products attract more fake reviews than others, and key words in the title may highlight these.\n",
    "\n",
    "We will use toFeatureVector to include these features in the information passed to predictLabels(). Rating is an int 1-5 so we can give this feature a weight of 1-5. Verified purchase is binary so will either be Y or N as per the rawData and product title will be pre-processed and included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseReviewQ5(reviewLine):\n",
    "    # reviewLine[0] = DOC_ID\n",
    "    # reviewLine[8] = REVIEW_TEXT\n",
    "    # reviewLine[1] = LABEL\n",
    "    # reviewLine[2] = RATING\n",
    "    # reviewLine[3] = VERIFIED_PURCHASE\n",
    "    # reviewLine[6] = PRODUCT_TITLE\n",
    "    return (reviewLine[0], reviewLine[8], reviewLine[1], reviewLine[2], reviewLine[3], reviewLine[6])\n",
    "    # return DOC_ID, REVIEW_TEXT, LABEL, RATING, VERIFIED_PURCHASE, PRODUCT_TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadDataQ5(path, Text=None):\n",
    "    with open(path, newline='', encoding=\"latin-1\") as f: \n",
    "        # use encoding=\"latin-1\" encoding to fix Unicode decode errors?\n",
    "        next(f) # skip the first line (headers)\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            #print(line)\n",
    "            (Id, Text, Label, Rating, Verified, Title) = parseReviewQ5(line)\n",
    "            rawData.append((Id, Text, Label, Rating, Verified, Title))\n",
    "            preprocessedData.append((Id, preProcess(Text), Label, Rating, Verified, preProcess(Title)))\n",
    "        print(\"All data loaded from \" + path)\n",
    "\n",
    "def processVectorQ5():\n",
    "    for (_, Text, Label, Rating, Verified, Title) in rawData:\n",
    "        vectoredProcessed.append((toFeatureVectorQ5(preProcess(Text), Rating, Verified, preProcess(Title)),Label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVectorQ5(text, rating, verified, title):\n",
    "\n",
    "    featureVector = {}\n",
    "\n",
    "    for token in text:\n",
    "        # feature dictionary\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1\n",
    "        else:\n",
    "            featureVector[token] += 1\n",
    "        # global dictionary\n",
    "        if token not in featureDict:\n",
    "            featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] += 1\n",
    "            \n",
    "    for token in title:\n",
    "        # feature dictionary\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1\n",
    "        else:\n",
    "            featureVector[token] += 1\n",
    "    \n",
    "    featureVector['rating'] = rating\n",
    "    featureVector['verified'] = verified\n",
    "    \n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData\n",
      "Preparing the dataset...\n",
      "All data loaded from amazon_reviews.txt\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "# reset the featuredict\n",
    "featureDict = {}\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "vectoredProcessed = []  # our completed data, with text to vectors and preprocessed\n",
    "\n",
    "# references to the data files\n",
    "#reviewPath = '100_reviews.txt'\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData\" % (len(rawData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadDataQ5(reviewPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to reset our processed data and featureDict after changing settings of toFeatureVector/preProcess()\n",
    "featureDict = {}\n",
    "vectoredProcessed = []\n",
    "\n",
    "processVectorQ5() # apply toFeatureVector() and preProcess() on the rawData, populate vectoredProcessed to cross validate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'least': 1, 'think': 1, 'product': 1, 'save': 1, 'day': 1, 'keep': 1, 'around': 1, 'case': 1, 'need': 1, 'someth': 1, 'targus': 1, 'pauk10u': 1, 'ultra': 1, 'mini': 1, 'usb': 1, 'keypad': 1, 'black': 1, 'rating': '4', 'verified': 'N'}, '__label1__')\n"
     ]
    }
   ],
   "source": [
    "print(vectoredProcessed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Average:  [0.8007384140998066, 0.8005714285714285, 0.8005548081465619]\n"
     ]
    }
   ],
   "source": [
    "cv_results = crossValidate(vectoredProcessed, 10)\n",
    "# print(cv_results)\n",
    "\n",
    "total = [0,0,0]\n",
    "for x in cv_results:\n",
    "    total[0] += x[0] # precision\n",
    "    total[1] += x[1] # recall\n",
    "    total[2] += x[2] # f score\n",
    "\n",
    "# averages\n",
    "total[0] = total[0]/len(cv_results)\n",
    "total[1] = total[1]/len(cv_results)\n",
    "total[2] = total[2]/len(cv_results)\n",
    "\n",
    "print(\"Average: \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example review encoding before being passed to predictLabel:\n",
    "\n",
    "`({'least': 1, 'think': 1, 'product': 1, 'save': 1, 'day': 1, 'keep': 1, 'around': 1, 'case': 1, 'need': 1, 'someth': 1, 'targus': 1, 'pauk10u': 1, 'ultra': 1, 'mini': 1, 'usb': 1, 'keypad': 1, 'black': 1, 'rating': '4', 'verified': 'N'}, '__label1__')`\n",
    "\n",
    "# Results\n",
    "\n",
    "> Average:  [0.8007384140998066, 0.8005714285714285, 0.8005548081465619]\n",
    "\n",
    "Here we saw a huge improvement in the fscore and predictive power. The conclusion from this is that the words alone do not have much predictive power in terms of predicting whether a review is fake. Whilst we still can predict better than chance with words alone (0.6 vs 0.5) it seems the majority of the predictive power comes from the other categories such as rating and verified purchase. This is likely because the fake reviews are generally quite good (although not perfect) at producing realistic textual features. The fake reviews are likely written mostly by real people hired to do so, rather than computer generated.\n",
    "\n",
    "I was interested to check my intuitions that fake reviews would have disproportionaly higher 5 star rating, and also that they were most likely non-verified purchases. As such I looked at these features as counts in the data provided by Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 star reviews        Fake:  6059  Real:  6151\n",
      "4 star reviews        Fake:  1999  Real:  1974\n",
      "3 star reviews        Fake:  926   Real:  942\n",
      "2 star reviews        Fake:  627   Real:  565\n",
      "1 star reviews        Fake:  889   Real:  868\n",
      "Unverified purchases  Fake:  7623  Real:  1679\n",
      "Verified purchases    Fake:  2877  Real:  8821\n"
     ]
    }
   ],
   "source": [
    "# rawData[2] = label\n",
    "# rawData[3] = rating\n",
    "# rawData[4] = verified\n",
    "# rawData[5] = category\n",
    "\n",
    "ratingsFake = {}\n",
    "ratingsReal = {}\n",
    "verifiedFake = {}\n",
    "verifiedReal = {}\n",
    "\n",
    "verifiedReal[\"N\"] = 0\n",
    "verifiedReal[\"Y\"] = 0\n",
    "\n",
    "verifiedFake[\"Y\"] = 0\n",
    "verifiedFake[\"N\"] = 0\n",
    "\n",
    "for line in rawData:\n",
    "    if line[2] == \"__label2__\": # real\n",
    "        if line[4] == \"N\": # unverified\n",
    "            verifiedReal[line[4]] += 1\n",
    "        if line[4] == \"Y\":\n",
    "            verifiedReal[line[4]] += 1\n",
    "        if line[3] not in ratingsReal:\n",
    "            ratingsReal[line[3]] = 1\n",
    "        else:\n",
    "            ratingsReal[line[3]] += 1\n",
    "            \n",
    "    if line[2] == \"__label1__\": # fake\n",
    "        if line[4] == \"N\": # unverified\n",
    "            verifiedFake[line[4]] += 1\n",
    "        if line[4] == \"Y\":\n",
    "            verifiedFake[line[4]] += 1\n",
    "        if line[3] not in ratingsFake:\n",
    "            ratingsFake[line[3]] = 1\n",
    "        else:\n",
    "            ratingsFake[line[3]] += 1\n",
    "        \n",
    "print(\"5 star reviews        Fake: \", ratingsFake[\"5\"], \" Real: \", ratingsReal[\"5\"])\n",
    "print(\"4 star reviews        Fake: \", ratingsFake[\"4\"], \" Real: \", ratingsReal[\"4\"])\n",
    "print(\"3 star reviews        Fake: \", ratingsFake[\"3\"], \"  Real: \", ratingsReal[\"3\"])\n",
    "print(\"2 star reviews        Fake: \", ratingsFake[\"2\"], \"  Real: \", ratingsReal[\"2\"])\n",
    "print(\"1 star reviews        Fake: \", ratingsFake[\"1\"], \"  Real: \", ratingsReal[\"1\"])\n",
    "print(\"Unverified purchases  Fake: \", verifiedFake[\"N\"], \" Real: \", verifiedReal[\"N\"])\n",
    "print(\"Verified purchases    Fake: \", verifiedFake[\"Y\"], \" Real: \", verifiedReal[\"Y\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Fake | Real |\n",
    "|--|------|------|\n",
    "|5 star reviews|6059|6151|\n",
    "|4 star reviews|1999|1974|\n",
    "|3 star reviews|926|942|\n",
    "|2 star reviews|627|565|\n",
    "|1 star reviews|889|868|\n",
    "|Unverified purchases|7623|1679|\n",
    "|Verified purchases|2877|8821|\n",
    "\n",
    "Contrary to our intuition the number of fake 5 star reviews is roughly equivilent in both the fake and real labels, and also roughly equivilent across all ratings. It seems like this alone may not predict much, although there may be some interactions with other features.\n",
    "\n",
    "We can see clearly that the fake reviews contain a much larger number of unverified purchases. Similarly, the real reviews are much more likely to have verified purchases. This is likely the best indicator in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
